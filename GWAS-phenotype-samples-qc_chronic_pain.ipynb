{"metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11.5", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Sample QC for GWAS analysis\n", "metadata": {}}, {"cell_type": "markdown", "source": "## Jupyterlab app details (launch configuration)\n\nRecommended configuration\n- runtime: < 10 min\n- cluster configuration: `Spark cluster`\n- number of nodes: 2\n- recommended instance: `mem1_ssd1_v2_x16`\n- cost: < \u00a30.09", "metadata": {}}, {"cell_type": "markdown", "source": "1. Import libraries and initialize Spark connection.", "metadata": {}}, {"cell_type": "code", "source": "import os\nimport pyspark.pandas as ks\nimport dxpy\nimport dxdata\nimport pandas as pd\nimport pyspark\nimport re\n\n# Set the environment variable\nos.environ['PYARROW_IGNORE_TIMEZONE'] = '1'\n\n# Initialize Spark\nsc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)", "metadata": {"trusted": true, "tags": []}, "execution_count": 1, "outputs": [{"name": "stderr", "text": "/cluster/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n  warnings.warn(\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "2. Secify whole exome data (WES) directory, exome field ID, these variables will depend on WES release (e.g. 200K, 300K or 450K) and output directory.", "metadata": {}}, {"cell_type": "code", "source": "exome_folder = 'Population level exome OQFE variants, PLINK format - final release'\nexome_field_id = '23158'\noutput_dir = '/'", "metadata": {"trusted": true, "tags": []}, "execution_count": 2, "outputs": []}, {"cell_type": "code", "source": "imputation_folder = 'Imputation from genotype (GEL)'\nimputation_field_id = '21008'", "metadata": {"trusted": true, "tags": []}, "execution_count": 3, "outputs": []}, {"cell_type": "markdown", "source": "3. Load daatset description and select entity containing phenotypic data.", "metadata": {}}, {"cell_type": "code", "source": "# Automatically discover cohort IDs by name\ncontrol_cohort_obj = dxpy.find_one_data_object(\n    typename=\"CohortBrowser\",\n    name=\"CPSP_NEW_CONTROLS\",\n    folder=\"/Cohorts\",\n    name_mode=\"exact\"\n)\ncontrol_cohort_id = control_cohort_obj[\"id\"]\n\ncase_cohort_obj = dxpy.find_one_data_object(\n    typename=\"CohortBrowser\",\n    name=\"CPSP_NEW_CASES\",\n    folder=\"/Cohorts\",\n    name_mode=\"exact\"\n)\ncase_cohort_id = case_cohort_obj[\"id\"]\n\n# Load the cohorts\ncontrol_cohort = dxdata.load_cohort(id=control_cohort_id)\ncase_cohort = dxdata.load_cohort(id=case_cohort_id)\n", "metadata": {"trusted": true, "tags": []}, "execution_count": 17, "outputs": []}, {"cell_type": "code", "source": "# Automatically discover dispensed dataset ID and load the dataset\ndispensed_dataset = dxpy.find_one_data_object(\n    typename=\"Dataset\", \n    name=\"app*.dataset\", \n    folder=\"/\", \n    name_mode=\"glob\"\n)\ndispensed_dataset_id = dispensed_dataset[\"id\"]\ndataset = dxdata.load_dataset(id=dispensed_dataset_id)", "metadata": {"trusted": true, "tags": []}, "execution_count": 18, "outputs": []}, {"cell_type": "code", "source": "participant = dataset['participant']", "metadata": {"trusted": true, "tags": []}, "execution_count": 6, "outputs": []}, {"cell_type": "markdown", "source": "4. Load cohorts that were created in cohort browser.", "metadata": {}}, {"cell_type": "code", "source": "\"\"\"\"\ncase = dxdata.load_cohort(id=\"record-J09vyy0JBBbK3k6YFB8BJ1qP\")\ncont = dxdata.load_cohort(id=\"record-J09vykQJBBbJ1b7KZX8qvf5Y\")\n\n\"\"\"\"", "metadata": {"trusted": true, "tags": []}, "execution_count": 7, "outputs": []}, {"cell_type": "code", "source": "case = case_cohort\ncont = control_cohort", "metadata": {"trusted": true, "tags": []}, "execution_count": 20, "outputs": []}, {"cell_type": "markdown", "source": "5. Specify fields ID to retrieve, get corresponding UKB RAP field names and print description table.", "metadata": {}}, {"cell_type": "code", "source": "field_ids = ['31', '22001', '22006', '22019', '34', '21022', '29100', '29011','23104', '22020', '2966',\n    '22009', '41270']", "metadata": {"trusted": true, "tags": []}, "execution_count": 9, "outputs": []}, {"cell_type": "code", "source": "def fields_for_id(field_id):\n    '''Collect field objects from UKB RAP based on field ID.'''\n    field_id = str(field_id)\n    fields = list(participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id)))\n\n    # Sort numerically if PCA\n    if field_id == '22009':\n        fields = sorted(\n            [f for f in fields if re.search(r'a(\\d+)', f.name)],\n            key=lambda f: int(re.search(r'a(\\d+)', f.name).group(1))\n        )\n        return fields[:10]\n    # Otherwise, return only first unless it's 2966\n    elif field_id != '2966' and len(fields) > 1:\n        return [fields[0]]\n    else:\n        return fields\n", "metadata": {"trusted": true, "tags": []}, "execution_count": 21, "outputs": []}, {"cell_type": "code", "source": "fields = []\nfor f in field_ids:\n    fs = fields_for_id(f)\n    if f == '22009':\n        fields.extend(fs)  # keep all 10 PCs\n    else:\n        fields.append(fs[0])  # just the first field\nfields += [participant.find_field(name='p20160_i0'), participant.find_field(name='eid')]\n\nfield_description = pd.DataFrame({\n    'Field': [f.name for f in fields],\n    'Title': [f.title for f in fields],\n    'Coding': [f.coding.codes if f.coding is not None else '' for f in fields ]\n})\n\nfield_description", "metadata": {"trusted": true, "tags": []}, "execution_count": 22, "outputs": [{"execution_count": 22, "output_type": "execute_result", "data": {"text/plain": "         Field                                              Title  \\\n0          p31                                                Sex   \n1       p22001                                        Genetic sex   \n2       p22006                            Genetic ethnic grouping   \n3       p22019                          Sex chromosome aneuploidy   \n4          p34                                      Year of birth   \n5       p21022                                 Age at recruitment   \n6       p29100  Ever had known person concerned about, or reco...   \n7       p29011  Ever had prolonged feelings of sadness or depr...   \n8    p23104_i0                 Body mass index (BMI) | Instance 0   \n9       p22020               Used in genetic principal components   \n10    p2966_i0     Age high blood pressure diagnosed | Instance 0   \n11   p22009_a1             Genetic principal components | Array 1   \n12   p22009_a2             Genetic principal components | Array 2   \n13   p22009_a3             Genetic principal components | Array 3   \n14   p22009_a4             Genetic principal components | Array 4   \n15   p22009_a5             Genetic principal components | Array 5   \n16   p22009_a6             Genetic principal components | Array 6   \n17   p22009_a7             Genetic principal components | Array 7   \n18   p22009_a8             Genetic principal components | Array 8   \n19   p22009_a9             Genetic principal components | Array 9   \n20  p22009_a10            Genetic principal components | Array 10   \n21      p41270                                  Diagnoses - ICD10   \n22   p20160_i0                           Ever smoked | Instance 0   \n23         eid                                     Participant ID   \n\n                                               Coding  \n0                        {'0': 'Female', '1': 'Male'}  \n1                        {'0': 'Female', '1': 'Male'}  \n2                                  {'1': 'Caucasian'}  \n3                                        {'1': 'Yes'}  \n4                                                      \n5                                                      \n6   {'-3': 'Prefer not to answer', '0': 'No', '1':...  \n7   {'1': 'Yes', '0': 'No', '-3': 'Prefer not to a...  \n8                                                      \n9                                        {'1': 'Yes'}  \n10  {'-1': 'Do not know', '-3': 'Prefer not to ans...  \n11                                                     \n12                                                     \n13                                                     \n14                                                     \n15                                                     \n16                                                     \n17                                                     \n18                                                     \n19                                                     \n20                                                     \n21  {'Chapter I': 'Chapter I Certain infectious an...  \n22                            {'1': 'Yes', '0': 'No'}  \n23                                                     ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Field</th>\n      <th>Title</th>\n      <th>Coding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>p31</td>\n      <td>Sex</td>\n      <td>{'0': 'Female', '1': 'Male'}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>p22001</td>\n      <td>Genetic sex</td>\n      <td>{'0': 'Female', '1': 'Male'}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>p22006</td>\n      <td>Genetic ethnic grouping</td>\n      <td>{'1': 'Caucasian'}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>p22019</td>\n      <td>Sex chromosome aneuploidy</td>\n      <td>{'1': 'Yes'}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>p34</td>\n      <td>Year of birth</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>p21022</td>\n      <td>Age at recruitment</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>p29100</td>\n      <td>Ever had known person concerned about, or reco...</td>\n      <td>{'-3': 'Prefer not to answer', '0': 'No', '1':...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>p29011</td>\n      <td>Ever had prolonged feelings of sadness or depr...</td>\n      <td>{'1': 'Yes', '0': 'No', '-3': 'Prefer not to a...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>p23104_i0</td>\n      <td>Body mass index (BMI) | Instance 0</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>p22020</td>\n      <td>Used in genetic principal components</td>\n      <td>{'1': 'Yes'}</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>p2966_i0</td>\n      <td>Age high blood pressure diagnosed | Instance 0</td>\n      <td>{'-1': 'Do not know', '-3': 'Prefer not to ans...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>p22009_a1</td>\n      <td>Genetic principal components | Array 1</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>p22009_a2</td>\n      <td>Genetic principal components | Array 2</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>p22009_a3</td>\n      <td>Genetic principal components | Array 3</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>p22009_a4</td>\n      <td>Genetic principal components | Array 4</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>p22009_a5</td>\n      <td>Genetic principal components | Array 5</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>p22009_a6</td>\n      <td>Genetic principal components | Array 6</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>p22009_a7</td>\n      <td>Genetic principal components | Array 7</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>p22009_a8</td>\n      <td>Genetic principal components | Array 8</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>p22009_a9</td>\n      <td>Genetic principal components | Array 9</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>p22009_a10</td>\n      <td>Genetic principal components | Array 10</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>p41270</td>\n      <td>Diagnoses - ICD10</td>\n      <td>{'Chapter I': 'Chapter I Certain infectious an...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>p20160_i0</td>\n      <td>Ever smoked | Instance 0</td>\n      <td>{'1': 'Yes', '0': 'No'}</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>eid</td>\n      <td>Participant ID</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"cell_type": "markdown", "source": "6. Retrieve data for both cohorts.", "metadata": {}}, {"cell_type": "code", "source": "# For case data\ncase_df = participant.retrieve_fields(fields=fields, filter_sql=case.sql, engine=dxdata.connect()).to_pandas_on_spark()\n\n# For control data\ncont_df = participant.retrieve_fields(fields=fields, filter_sql=cont.sql, engine=dxdata.connect(\n    dialect=\"hive+pyspark\", \n    connect_args={\n        'config': {\n            'spark.kryoserializer.buffer.max': '256m', \n            'spark.sql.autoBroadcastJoinThreshold': '-1'\n        }\n    }\n)).to_pandas_on_spark()\n", "metadata": {"trusted": true, "tags": []}, "execution_count": null, "outputs": [{"name": "stderr", "text": "/opt/conda/lib/python3.11/site-packages/thrift/transport/TSSLSocket.py:53: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n  self._context = ssl.SSLContext(ssl_version)\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "7. Create phenotype variable and concatenate cohorts into one dataframe.", "metadata": {}}, {"cell_type": "code", "source": "case_df['chronic_pain_cc'] = 1\ncont_df['chronic_pain_cc'] = 0\n", "metadata": {"trusted": true, "tags": []}, "execution_count": 13, "outputs": []}, {"cell_type": "code", "source": "df = ks.concat([case_df, cont_df])", "metadata": {"trusted": true, "tags": []}, "execution_count": 14, "outputs": []}, {"cell_type": "code", "source": "df.shape", "metadata": {"trusted": true, "tags": []}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "df.chronic_pain_cc.value_counts()", "metadata": {"trusted": true, "tags": []}, "execution_count": 16, "outputs": [{"name": "stderr", "text": "/cluster/spark/python/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n  warnings.warn(\n[root] ERROR: KeyboardInterrupt while sending command.\nTraceback (most recent call last):\n  File \"/cluster/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n", "output_type": "stream"}, {"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mchronic_pain_cc\u001b[38;5;241m.\u001b[39mvalue_counts()\n", "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/displayhook.py:268\u001b[0m, in \u001b[0;36mDisplayHook.__call__\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_displayhook()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_output_prompt()\n\u001b[0;32m--> 268\u001b[0m format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_format_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_user_ns(result)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_exec_result(result)\n", "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/displayhook.py:157\u001b[0m, in \u001b[0;36mDisplayHook.compute_format_data\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_format_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, result):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute format data of the object to be displayed.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    The format data is a generalization of the :func:`repr` of an object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    177\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n", "File \u001b[0;32m<decorator-gen-3>:2\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n", "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n", "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n", "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n", "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(obj)\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n", "File \u001b[0;32m/cluster/spark/python/pyspark/pandas/series.py:7342\u001b[0m, in \u001b[0;36mSeries.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   7337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_display_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   7338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_internal_pandas()\u001b[38;5;241m.\u001b[39mto_string(\n\u001b[1;32m   7339\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   7340\u001b[0m     )\n\u001b[0;32m-> 7342\u001b[0m pser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_psdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_repr_pandas_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_display_count\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname]\n\u001b[1;32m   7343\u001b[0m pser_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pser)\n\u001b[1;32m   7344\u001b[0m pser \u001b[38;5;241m=\u001b[39m pser\u001b[38;5;241m.\u001b[39miloc[:max_display_count]\n", "File \u001b[0;32m/cluster/spark/python/pyspark/pandas/frame.py:13393\u001b[0m, in \u001b[0;36mDataFrame._get_or_create_repr_pandas_cache\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m  13390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_or_create_repr_pandas_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries]:\n\u001b[1;32m  13391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_repr_pandas_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_pandas_cache:\n\u001b[1;32m  13392\u001b[0m         \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\n\u001b[0;32m> 13393\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_repr_pandas_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, {n: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_internal_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m  13394\u001b[0m         )\n\u001b[1;32m  13395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_pandas_cache[n]\n", "File \u001b[0;32m/cluster/spark/python/pyspark/pandas/frame.py:13388\u001b[0m, in \u001b[0;36mDataFrame._to_internal_pandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  13382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_to_internal_pandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m  13383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  13384\u001b[0m \u001b[38;5;124;03m    Return a pandas DataFrame directly from _internal to avoid overhead of copy.\u001b[39;00m\n\u001b[1;32m  13385\u001b[0m \n\u001b[1;32m  13386\u001b[0m \u001b[38;5;124;03m    This method is for internal use only.\u001b[39;00m\n\u001b[1;32m  13387\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 13388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas_frame\u001b[49m\n", "File \u001b[0;32m/cluster/spark/python/pyspark/pandas/utils.py:600\u001b[0m, in \u001b[0;36mlazy_property.<locals>.wrapped_lazy_property\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_lazy_property\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[0;32m--> 600\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name, \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name)\n", "File \u001b[0;32m/cluster/spark/python/pyspark/pandas/internal.py:1115\u001b[0m, in \u001b[0;36mInternalFrame.to_pandas_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return as pandas DataFrame.\"\"\"\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_internal_spark_frame\n\u001b[0;32m-> 1115\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sdf\u001b[38;5;241m.\u001b[39mschema) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1117\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pdf\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m   1118\u001b[0m         {field\u001b[38;5;241m.\u001b[39mname: spark_type_to_pandas_dtype(field\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m sdf\u001b[38;5;241m.\u001b[39mschema}\n\u001b[1;32m   1119\u001b[0m     )\n", "File \u001b[0;32m/cluster/spark/python/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n", "File \u001b[0;32m/cluster/spark/python/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n", "File \u001b[0;32m/cluster/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n", "File \u001b[0;32m/cluster/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n", "File \u001b[0;32m/cluster/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n", "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n", "\u001b[0;31mKeyboardInterrupt\u001b[0m: "], "ename": "KeyboardInterrupt", "evalue": "", "output_type": "error"}]}, {"cell_type": "markdown", "source": "8. QC samples based on several conditions.", "metadata": {}}, {"cell_type": "code", "source": "import numpy as np\nfrom scipy.stats import chi2_contingency\n\n# Start with original count\noriginal_count = len(df)\noriginal_cases = df[df.chronic_pain_cc == 1].shape[0]\noriginal_controls = df[df.chronic_pain_cc == 0].shape[0]\nprint(f\"Original participants: {original_count} (Cases: {original_cases}, Controls: {original_controls})\")\n\n# Function to track filtering impact on cases and controls with chi-square test\ndef track_filter_impact(filter_name, filter_condition, previous_filter=None):\n    if previous_filter is not None:\n        combined_filter = previous_filter & filter_condition\n    else:\n        combined_filter = filter_condition\n    \n    filtered_df = df[combined_filter]\n    total_count = len(filtered_df)\n    \n    cases_count = filtered_df[filtered_df.chronic_pain_cc == 1].shape[0]\n    controls_count = filtered_df[filtered_df.chronic_pain_cc == 0].shape[0]\n    \n    if previous_filter is not None:\n        previous_df = df[previous_filter]\n        previous_total = len(previous_df)\n        previous_cases = previous_df[previous_df.chronic_pain_cc == 1].shape[0]\n        previous_controls = previous_df[previous_df.chronic_pain_cc == 0].shape[0]\n        \n        cases_removed = previous_cases - cases_count\n        controls_removed = previous_controls - controls_count\n        total_removed = previous_total - total_count\n        \n        cases_percent_removed = cases_removed / previous_cases if previous_cases > 0 else 0\n        controls_percent_removed = controls_removed / previous_controls if previous_controls > 0 else 0\n        total_percent_removed = total_removed / previous_total if previous_total > 0 else 0\n        \n        print(f\"\\n--- After {filter_name} ---\")\n        print(f\"Total: {total_count} (Removed: {total_removed}, {total_percent_removed:.2%})\")\n        print(f\"Cases: {cases_count} (Removed: {cases_removed}, {cases_percent_removed:.2%})\")\n        print(f\"Controls: {controls_count} (Removed: {controls_removed}, {controls_percent_removed:.2%})\")\n        \n        # Chi-square test for this step only\n        if cases_removed > 0 or controls_removed > 0:\n            contingency = [\n                [cases_removed, controls_removed],\n                [cases_count, controls_count]\n            ]\n            \n            # Check if expected frequencies are all >= 5\n            chi2, p, dof, expected = chi2_contingency(contingency)\n            \n            min_expected = np.min(expected)\n            if min_expected < 5:\n                print(f\"Warning: Chi-square may not be valid (min expected frequency: {min_expected:.2f} < 5)\")\n            \n            print(f\"Chi-square test for this step:\")\n            print(f\"Chi2 value: {chi2:.4f}\")\n            print(f\"p-value: {p:.6f}\")\n            print(f\"{'*' * 3 if p < 0.001 else '*' * 2 if p < 0.01 else '*' if p < 0.05 else 'ns'} {'Significant difference' if p < 0.05 else 'No significant difference'} in filtering effect\")\n    else:\n        print(f\"\\n--- After {filter_name} ---\")\n        print(f\"Total: {total_count}\")\n        print(f\"Cases: {cases_count}\")\n        print(f\"Controls: {controls_count}\")\n    \n    # Also perform chi-square against the original dataset\n    if original_cases > 0 and original_controls > 0:\n        cases_removed_from_original = original_cases - cases_count\n        controls_removed_from_original = original_controls - controls_count\n        \n        contingency = [\n            [cases_removed_from_original, controls_removed_from_original],\n            [cases_count, controls_count]\n        ]\n        \n        chi2, p, dof, expected = chi2_contingency(contingency)\n        \n        min_expected = np.min(expected)\n        if min_expected < 5:\n            print(f\"Warning: Cumulative chi-square may not be valid (min expected frequency: {min_expected:.2f} < 5)\")\n        \n        print(f\"Cumulative chi-square test (compared to original):\")\n        print(f\"Chi2 value: {chi2:.4f}\")\n        print(f\"p-value: {p:.6f}\")\n        print(f\"{'*' * 3 if p < 0.001 else '*' * 2 if p < 0.01 else '*' if p < 0.05 else 'ns'} {'Significant difference' if p < 0.05 else 'No significant difference'} in cumulative filtering effect\")\n    \n    return combined_filter\n\n# Apply each filter and track impact\nfilter1 = track_filter_impact(\"filtering for same sex and genetic sex\", \n                             df['p31'] == df['p22001'])\n\nfilter2 = track_filter_impact(\"filtering for Caucasian ethnic grouping\", \n                             df['p22006'] == 1, \n                             filter1)\n\nfilter3 = track_filter_impact(\"filtering for no sex chromosome aneuploidy\", \n                             df['p22019'].isnull(), \n                             filter2)\n\nfilter4 = track_filter_impact(\"filtering for participants were used to calculate PCA (only non-relatives were included)\", \n                              df['p22020'] == 1, filter3)\n\n\n# Final filtered dataset\ndf_qced = df[filter4]\n\n# Overall summary\nprint(\"\\n=== OVERALL SUMMARY ===\")\nfinal_cases = df_qced[df_qced.chronic_pain_cc == 1].shape[0]\nfinal_controls = df_qced[df_qced.chronic_pain_cc == 0].shape[0]\n\ncases_removed_total = original_cases - final_cases\ncontrols_removed_total = original_controls - final_controls\ntotal_removed = original_count - len(df_qced)\n\ncases_percent_remaining = final_cases / original_cases\ncontrols_percent_remaining = final_controls / original_controls\ntotal_percent_remaining = len(df_qced) / original_count\n\nprint(f\"Original participants: {original_count} (Cases: {original_cases}, Controls: {original_controls})\")\nprint(f\"Final participants: {len(df_qced)} (Cases: {final_cases}, Controls: {final_controls})\")\nprint(f\"Total removed: {total_removed} ({1-total_percent_remaining:.2%})\")\nprint(f\"Cases removed: {cases_removed_total} ({1-cases_percent_remaining:.2%})\")\nprint(f\"Controls removed: {controls_removed_total} ({1-controls_percent_remaining:.2%})\")\n\n# Case-control ratio before and after\noriginal_ratio = original_cases / original_controls if original_controls > 0 else float('inf')\nfinal_ratio = final_cases / final_controls if final_controls > 0 else float('inf')\nprint(f\"\\nCase-to-control ratio before: 1:{1/original_ratio:.4f}\")\nprint(f\"Case-to-control ratio after: 1:{1/final_ratio:.4f}\")", "metadata": {"trusted": true, "tags": []}, "execution_count": 56, "outputs": [{"name": "stdout", "text": "Original participants: 111243 (Cases: 5696, Controls: 105547)\n\n--- After filtering for same sex and genetic sex ---\nTotal: 108827\nCases: 5552\nControls: 103275\nCumulative chi-square test (compared to original):\nChi2 value: 3.4119\np-value: 0.064729\nns No significant difference in cumulative filtering effect\n\n--- After filtering for Caucasian ethnic grouping ---\nTotal: 93682 (Removed: 15145, 13.92%)\nCases: 4808 (Removed: 744, 13.40%)\nControls: 88874 (Removed: 14401, 13.94%)\nChi-square test for this step:\nChi2 value: 1.2553\np-value: 0.262538\nns No significant difference in filtering effect\nCumulative chi-square test (compared to original):\nChi2 value: 0.1588\np-value: 0.690307\nns No significant difference in cumulative filtering effect\n\n--- After filtering for no sex chromosome aneuploidy ---\nTotal: 93608 (Removed: 74, 0.08%)\nCases: 4803 (Removed: 5, 0.10%)\nControls: 88805 (Removed: 69, 0.08%)\nWarning: Chi-square may not be valid (min expected frequency: 3.80 < 5)\nChi-square test for this step:\nChi2 value: 0.1369\np-value: 0.711345\nns No significant difference in filtering effect\nCumulative chi-square test (compared to original):\nChi2 value: 0.1244\np-value: 0.724346\nns No significant difference in cumulative filtering effect\n\n--- After filtering for participants were used to calculate PCA (only non-relatives were included) ---\nTotal: 78051 (Removed: 15557, 16.62%)\nCases: 4010 (Removed: 793, 16.51%)\nControls: 74041 (Removed: 14764, 16.63%)\nChi-square test for this step:\nChi2 value: 0.0354\np-value: 0.850839\nns No significant difference in filtering effect\nCumulative chi-square test (compared to original):\nChi2 value: 0.1502\np-value: 0.698312\nns No significant difference in cumulative filtering effect\n\n=== OVERALL SUMMARY ===\nOriginal participants: 111243 (Cases: 5696, Controls: 105547)\nFinal participants: 78051 (Cases: 4010, Controls: 74041)\nTotal removed: 33192 (29.84%)\nCases removed: 1686 (29.60%)\nControls removed: 31506 (29.85%)\n\nCase-to-control ratio before: 1:18.5300\nCase-to-control ratio after: 1:18.4641\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "\"\"\"\n# Apply filters based on the descriptions you provided\ndf_qced = df[\n    (df['p31'] == df['p22001']) &  # Filter for same sex and genetic sex\n    (df['p22006'] == 1) &          # Caucasian ethnic grouping\n    (df['p22019'].isnull()) &       # No sex chromosome aneuploidy\n    (df['p22021'] == 0)             # No kinship found\n]\n\n\"\"\"", "metadata": {"trusted": true, "tags": []}, "execution_count": 14, "outputs": []}, {"cell_type": "code", "source": "df_qced = df_qced.rename(columns=lambda x: re.sub('p22009_a','pc',x))\n# Rename the 'eid' column to 'IID' along with other relevant columns\ndf_qced = df_qced.rename(columns={\n    'eid': 'IID',  # Rename 'eid' to 'IID'\n    'p31': 'sex',\n    'p34': 'year_of_birth',\n    'p21022': 'age_at_recruitment',\n    'p22001': 'genetic_sex',  # Rename p22001 to genetic_sex\n    'p20160_i0': 'ever_smoked',\n    'p22006': 'ethnic_group',  # Genetic ethnic grouping (Caucasian)\n    'p22019': 'sex_chromosome_aneuploidy',  # Sex chromosome aneuploidy\n    'p22021': 'kinship_to_other_participants',  # Kinship status\n    'p29100': 'known_person_concerned_about_alcohol',  # Ever had known person concerned about alcohol consumption\n    'p29011': 'ever_had_prolonged_sadness_or_depression',  # Ever had prolonged feelings of sadness or depression\n    'p23104_i0': 'BMI',\n    'p2966_i0': 'age_diagnosed_htn', # Age high blood pressure diagnosed\n    'p22020': 'used in genetic principal components',\n    'p41270': 'diagnoses' #ICD10\n    \n})\n\n", "metadata": {"trusted": true, "tags": []}, "execution_count": 61, "outputs": []}, {"cell_type": "code", "source": "# Check the columns of df_qced to ensure 'IID' exists\nprint(df_qced.columns)", "metadata": {"trusted": true, "tags": []}, "execution_count": 66, "outputs": [{"name": "stdout", "text": "Index(['sex', 'genetic_sex', 'ethnic_group', 'sex_chromosome_aneuploidy',\n       'year_of_birth', 'age_at_recruitment',\n       'known_person_concerned_about_alcohol',\n       'ever_had_prolonged_sadness_or_depression', 'BMI',\n       'used in genetic principal components', 'age_diagnosed_htn', 'pc1',\n       'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8', 'pc9', 'pc10',\n       'diagnoses', 'ever_smoked', 'IID', 'chronic_pain_cc'],\n      dtype='object')\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "#Filling Nans", "metadata": {}}, {"cell_type": "code", "source": "# Assign 'FID' from 'IID'\ndf_qced['FID'] = df_qced['IID']", "metadata": {"trusted": true, "tags": []}, "execution_count": 67, "outputs": []}, {"cell_type": "code", "source": "# Fill missing in binary/categorical with 0 or mode\ndf_qced['ever_smoked'].fillna(0, inplace=True)\ndf_qced['known_person_concerned_about_alcohol'].fillna(0, inplace=True)\ndf_qced['ever_had_prolonged_sadness_or_depression'].fillna(0, inplace=True)\n\n", "metadata": {"trusted": true, "tags": []}, "execution_count": 69, "outputs": []}, {"cell_type": "code", "source": "import numpy as np\n# Fill continuous variables with mean\ncontinuous_cols = [\n    'year_of_birth',\n    'age_at_recruitment',\n    'age_diagnosed_htn',\n    'BMI'\n]\n\nfor col in continuous_cols:\n    if col in df_qced.columns:\n        df_qced[col].fillna(df_qced[col].mean(), inplace=True)", "metadata": {"trusted": true, "tags": []}, "execution_count": 70, "outputs": []}, {"cell_type": "markdown", "source": "9. Rename columns and organize it in format suitable for PLINK and regenie.", "metadata": {}}, {"cell_type": "code", "source": "# Create a phenotype table from the QCed data\ndf_phenotype = df_qced[['FID', 'IID', 'chronic_pain_cc', 'sex', 'year_of_birth', 'age_at_recruitment', 'age_diagnosed_htn', 'ever_smoked',\n       'known_person_concerned_about_alcohol',\n       'ever_had_prolonged_sadness_or_depression', 'BMI', 'pc1',\n       'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8', 'pc9', 'pc10', 'diagnoses'\n       ]]\n\n# Display the phenotype table\ndf_phenotype.head()\n", "metadata": {"trusted": true, "tags": []}, "execution_count": 71, "outputs": [{"execution_count": 71, "output_type": "execute_result", "data": {"text/plain": "       FID      IID  chronic_pain_cc  sex  year_of_birth  age_at_recruitment  age_diagnosed_htn  ever_smoked  known_person_concerned_about_alcohol  ever_had_prolonged_sadness_or_depression       BMI      pc1      pc2      pc3      pc4       pc5       pc6       pc7      pc8      pc9      pc10                                                                                                                                                                                                              diagnoses\n0  1002940  1002940                1    1         1940.0                67.0          46.551462            1                                     0                                         0  24.90000 -13.9614  3.53320 -2.90706  6.88338  10.41120 -3.340360  2.710350 -2.17131  2.97747 -0.286813                                                                                                    [C61, J459, J61, K20, K219, K221, K227, K269, K297, K298, K30, K319, K449, K529, K589, K900, L721, M169, M179, R12]\n1  1003285  1003285                1    1         1942.0                67.0          46.551462            1                                     0                                         0  27.70000 -12.2651  1.76183 -3.59340  3.73171  -2.90846 -1.280710 -0.342788 -2.14150  3.24838  4.263460                                                                                                                                            [C61, D508, H919, I849, K210, K227, K29, K317, K590, K625, K635, R17, Z861]\n2  1007944  1007944                1    0         1948.0                60.0          54.000000            1                                     0                                         1  26.94923 -10.5345  3.31250 -1.95293 -3.08537  -3.38068  0.040655  0.388952 -2.38331 -4.92998  2.265460  [G454, G560, G610, G819, G822, H819, H830, I10, K269, M1993, M2137, M2556, M4787, M549, M819, M8587, N312, N819, N840, N950, R030, R040, R11, R42, S007, S5250, T830, W050, Z501, Z602, Z751, Z880, Z885, Z886, Z924]\n4  1014662  1014662                1    0         1946.0                61.0          46.551462            0                                     0                                         0  31.00000 -13.3167  5.80744 -1.20050  6.71599  16.02190  0.389076  2.498870  5.86578  1.46635  0.534134                                                                                                                                                                                                                   None\n5  1015654  1015654                1    1         1944.0                63.0          60.000000            1                                     0                                         0  36.60000 -11.4985  6.28084 -2.47919  3.19450   3.82175 -0.677714 -1.440480  0.83640  3.68877 -4.000730                                                                              [A419, C446, E119, E872, F059, I10, L405, M073, M0730, M0736, M0739, M170, N179, N390, N40, N428, R410, R471, R53, R69, R739, Z115, Z966]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FID</th>\n      <th>IID</th>\n      <th>chronic_pain_cc</th>\n      <th>sex</th>\n      <th>year_of_birth</th>\n      <th>age_at_recruitment</th>\n      <th>age_diagnosed_htn</th>\n      <th>ever_smoked</th>\n      <th>known_person_concerned_about_alcohol</th>\n      <th>ever_had_prolonged_sadness_or_depression</th>\n      <th>BMI</th>\n      <th>pc1</th>\n      <th>pc2</th>\n      <th>pc3</th>\n      <th>pc4</th>\n      <th>pc5</th>\n      <th>pc6</th>\n      <th>pc7</th>\n      <th>pc8</th>\n      <th>pc9</th>\n      <th>pc10</th>\n      <th>diagnoses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1002940</td>\n      <td>1002940</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1940.0</td>\n      <td>67.0</td>\n      <td>46.551462</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24.90000</td>\n      <td>-13.9614</td>\n      <td>3.53320</td>\n      <td>-2.90706</td>\n      <td>6.88338</td>\n      <td>10.41120</td>\n      <td>-3.340360</td>\n      <td>2.710350</td>\n      <td>-2.17131</td>\n      <td>2.97747</td>\n      <td>-0.286813</td>\n      <td>[C61, J459, J61, K20, K219, K221, K227, K269, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1003285</td>\n      <td>1003285</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1942.0</td>\n      <td>67.0</td>\n      <td>46.551462</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27.70000</td>\n      <td>-12.2651</td>\n      <td>1.76183</td>\n      <td>-3.59340</td>\n      <td>3.73171</td>\n      <td>-2.90846</td>\n      <td>-1.280710</td>\n      <td>-0.342788</td>\n      <td>-2.14150</td>\n      <td>3.24838</td>\n      <td>4.263460</td>\n      <td>[C61, D508, H919, I849, K210, K227, K29, K317,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1007944</td>\n      <td>1007944</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1948.0</td>\n      <td>60.0</td>\n      <td>54.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>26.94923</td>\n      <td>-10.5345</td>\n      <td>3.31250</td>\n      <td>-1.95293</td>\n      <td>-3.08537</td>\n      <td>-3.38068</td>\n      <td>0.040655</td>\n      <td>0.388952</td>\n      <td>-2.38331</td>\n      <td>-4.92998</td>\n      <td>2.265460</td>\n      <td>[G454, G560, G610, G819, G822, H819, H830, I10...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1014662</td>\n      <td>1014662</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1946.0</td>\n      <td>61.0</td>\n      <td>46.551462</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>31.00000</td>\n      <td>-13.3167</td>\n      <td>5.80744</td>\n      <td>-1.20050</td>\n      <td>6.71599</td>\n      <td>16.02190</td>\n      <td>0.389076</td>\n      <td>2.498870</td>\n      <td>5.86578</td>\n      <td>1.46635</td>\n      <td>0.534134</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1015654</td>\n      <td>1015654</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1944.0</td>\n      <td>63.0</td>\n      <td>60.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36.60000</td>\n      <td>-11.4985</td>\n      <td>6.28084</td>\n      <td>-2.47919</td>\n      <td>3.19450</td>\n      <td>3.82175</td>\n      <td>-0.677714</td>\n      <td>-1.440480</td>\n      <td>0.83640</td>\n      <td>3.68877</td>\n      <td>-4.000730</td>\n      <td>[A419, C446, E119, E872, F059, I10, L405, M073...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "df_phenotype.chronic_pain_cc.value_counts()", "metadata": {"trusted": true, "tags": []}, "execution_count": null, "outputs": [{"name": "stderr", "text": "/cluster/spark/python/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n  warnings.warn(\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "df_phenotype = df_phenotype.to_pandas()", "metadata": {"trusted": true, "tags": []}, "execution_count": 34, "outputs": [{"name": "stderr", "text": "/cluster/spark/python/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "df_no_qc = df_no_qc.to_pandas()", "metadata": {"trusted": true, "tags": []}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "10. Select only samples that have WES data available and save them to CSV file.", "metadata": {}}, {"cell_type": "code", "source": "# Define the base path and file naming convention\nexome_folder = \"/Bulk/Exome sequences/Population level exome OQFE variants, PLINK format - final release\"\nbase_filename = \"ukb23158_c\"", "metadata": {"trusted": true, "tags": []}, "execution_count": 32, "outputs": []}, {"cell_type": "code", "source": "# List of chromosomes to process (1 to 22)\nchromosomes = list(range(1, 23))\n\n# Placeholder for the combined dataframe\ncombined_df = pd.DataFrame()\n\n# Loop through each chromosome and process the .fam files\nfor chrom in chromosomes:\n    fam_file = f\"{exome_folder}/{base_filename}{chrom}_b0_v1.fam\"\n    \n    try:\n        # Check if the .fam file exists using dxpy\n        file_obj = dxpy.find_one_data_object(name=f\"{base_filename}{chrom}_b0_v1.fam\", folder=exome_folder, name_mode='exact')\n\n        if file_obj:\n            print(f\"Processing chromosome {chrom}\")\n            \n            # Get file ID and download it to a local file\n            fam_file_id = file_obj[\"id\"]\n            local_filename = f\"local_{base_filename}{chrom}_b0_v1.fam\"\n            \n            # Download the file locally\n            dxpy.download_dxfile(fam_file_id, local_filename)\n\n            # Load the downloaded .fam file using pandas\n            plink_fam_df = pd.read_csv(local_filename, delimiter='\\s', dtype='object', \n                                       names=['FID', 'IID', 'Father ID', 'Mother ID', 'sex', 'Pheno'], engine='python')\n\n            # Merge with phenotype data (assuming df_phenotype is already defined)\n            chromosome_df = pd.merge(df_phenotype, plink_fam_df[['IID']], on='IID', how='inner')\n\n            # Combine the data for all chromosomes\n            combined_df = pd.concat([combined_df, chromosome_df])\n\n            # Clean up: delete the local .fam file after processing\n            os.remove(local_filename)\n\n    except dxpy.DXError as e:\n        print(f\"File for chromosome {chrom} not found or an error occurred: {e}\")\n\n# Remove duplicates based on 'IID' to avoid counting the same individual multiple times\ncombined_df = combined_df.drop_duplicates(subset=['IID'])\n\n", "metadata": {"trusted": true, "tags": []}, "execution_count": 33, "outputs": [{"name": "stdout", "text": "Processing chromosome 1\nProcessing chromosome 2\nProcessing chromosome 3\nProcessing chromosome 4\nProcessing chromosome 5\nProcessing chromosome 6\nProcessing chromosome 7\nProcessing chromosome 8\nProcessing chromosome 9\nProcessing chromosome 10\nProcessing chromosome 11\nProcessing chromosome 12\nProcessing chromosome 13\nProcessing chromosome 14\nProcessing chromosome 15\nProcessing chromosome 16\nProcessing chromosome 17\nProcessing chromosome 18\nProcessing chromosome 19\nProcessing chromosome 20\nProcessing chromosome 21\nProcessing chromosome 22\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "combined_df.chronic_pain_cc.value_counts()", "metadata": {"trusted": true, "tags": []}, "execution_count": 27, "outputs": [{"execution_count": 27, "output_type": "execute_result", "data": {"text/plain": "chronic_pain_cc\n0    71195\n1     3861\nName: count, dtype: int64"}, "metadata": {}}]}, {"cell_type": "code", "source": "# Get imputed data\npath_to_impute_file = f'/mnt/project/REGENIE_output/{imputation_folder}/ukb{imputation_field_id}_c1_b0_v1.sample'\nsample_file = pd.read_csv(\n    path_to_impute_file,\n    delimiter='\\s',\n    header=0,\n    names=['FID', 'IID', 'missing', 'sex'],\n    engine='python',\n)\n\n# Check the data types\nprint(\"combined_df['IID'] dtype:\", df_phenotype['IID'].dtype)\nprint(\"sample_file['IID'] dtype:\", sample_file['IID'].dtype)\n\n# Convert IID columns to the same type (string)\ndf_phenotype['IID'] = df_phenotype['IID'].astype(str)\nsample_file['IID'] = sample_file['IID'].astype(str)\n\n# Now try the join again\ncpsp_df = df_phenotype.join(\n    sample_file.set_index('IID'), on='IID', rsuffix='_sample', how='inner'\n)\n\n# Drop unuseful columns from .fam file\ncpsp_df.drop(\n    columns=['FID_sample', 'missing', 'sex_sample'],\n    axis=1,\n    inplace=True,\n    errors='ignore',\n)", "metadata": {"trusted": true, "tags": []}, "execution_count": 38, "outputs": [{"name": "stdout", "text": "combined_df['IID'] dtype: object\nsample_file['IID'] dtype: int64\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "cpsp_df.chronic_pain_cc.value_counts()", "metadata": {"trusted": true, "tags": []}, "execution_count": 39, "outputs": [{"execution_count": 39, "output_type": "execute_result", "data": {"text/plain": "chronic_pain_cc\n0    74038\n1     4010\nName: count, dtype: int64"}, "metadata": {}}]}, {"cell_type": "code", "source": "# Save the combined phenotype data locally\noutput_filename = \"cpsp.phe\"\n\n# Save the file in your local working environment\ncpsp_df.to_csv(output_filename, sep='\\t', na_rep='NA', index=False)\nprint(f\"Saved combined file locally as {output_filename}\")\n\n# Define the destination path on DNAnexus\nremote_dir = '/Data/'", "metadata": {"trusted": true, "tags": []}, "execution_count": 40, "outputs": [{"name": "stdout", "text": "Saved combined file locally as cpsp.phe\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "# Save the combined phenotype data locally\noutput_filename_no_QC = \"no_sample_QC_chronic_pain_wes.phe\"\n\n# Save the file in your local working environment\ndf_no_qc.to_csv(output_filename_no_QC, sep='\\t', na_rep='NA', index=False)\nprint(f\"Saved combined file locally as {output_filename_no_QC}\")\n\n# Define the destination path on DNAnexus\nremote_dir = '/Data/'", "metadata": {"trusted": true, "tags": []}, "execution_count": 29, "outputs": [{"name": "stdout", "text": "Saved combined file locally as no_sample_QC_chronic_pain_wes.phe\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "11. Load file to project storage.", "metadata": {}}, {"cell_type": "code", "source": "# Upload the local file to the DNAnexus platform\ndxpy.upload_local_file(output_filename_no_QC, folder=remote_dir)\n\nprint(f\"Uploaded {output_filename_no_QC} to {remote_dir} on DNAnexus.\")", "metadata": {"trusted": true, "tags": []}, "execution_count": 30, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Upload the local file to the DNAnexus platform\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dxpy\u001b[38;5;241m.\u001b[39mupload_local_file(\u001b[43moutput_filename_no_QC\u001b[49m, folder\u001b[38;5;241m=\u001b[39mremote_dir)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_filename_no_QC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremote_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on DNAnexus.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n", "\u001b[0;31mNameError\u001b[0m: name 'output_filename_no_QC' is not defined"], "ename": "NameError", "evalue": "name 'output_filename_no_QC' is not defined", "output_type": "error"}]}, {"cell_type": "code", "source": "# Upload the local file to the DNAnexus platform\ndxpy.upload_local_file(output_filename, folder=remote_dir)\n\nprint(f\"Uploaded {output_filename} to {remote_dir} on DNAnexus.\")", "metadata": {"trusted": true, "tags": []}, "execution_count": 42, "outputs": [{"name": "stdout", "text": "Uploaded cpsp.phe to /Data/ on DNAnexus.\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "Here is an example of phenotype file:", "metadata": {"tags": []}}, {"cell_type": "code", "source": "# Path to the saved phenotypic file\nphenotypic_file_path = \"cpsp.phe\"\n\n# Load the phenotypic file into a pandas DataFrame\nphenotypic_df = pd.read_csv(phenotypic_file_path, delimiter='\\t')\n\n# Display the first few rows of the DataFrame\nphenotypic_df.head()", "metadata": {"trusted": true}, "execution_count": 41, "outputs": [{"execution_count": 41, "output_type": "execute_result", "data": {"text/plain": "       FID      IID  chronic_pain_cc  sex  year_of_birth  Age_at_recruitment  \\\n0  1002940  1002940                1    1         1940.0                67.0   \n1  1003285  1003285                1    1         1942.0                67.0   \n2  1007944  1007944                1    0         1948.0                60.0   \n3  1014662  1014662                1    0         1946.0                61.0   \n4  1015654  1015654                1    1         1944.0                63.0   \n\n   ever_smoked  known_person_concerned_about_alcohol  \\\n0            1                                     0   \n1            1                                     0   \n2            1                                     0   \n3            0                                     0   \n4            1                                     0   \n\n   ever_had_prolonged_sadness_or_depression       BMI  Genetic PCA  \\\n0                                         0  24.90000     -13.9614   \n1                                         0  27.70000     -12.2651   \n2                                         1  26.94923     -10.5345   \n3                                         0  31.00000     -13.3167   \n4                                         0  36.60000     -11.4985   \n\n                                           Diagnoses  \n0  ['C61', 'J459', 'J61', 'K20', 'K219', 'K221', ...  \n1  ['C61', 'D508', 'H919', 'I849', 'K210', 'K227'...  \n2  ['G454', 'G560', 'G610', 'G819', 'G822', 'H819...  \n3                                                NaN  \n4  ['A419', 'C446', 'E119', 'E872', 'F059', 'I10'...  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FID</th>\n      <th>IID</th>\n      <th>chronic_pain_cc</th>\n      <th>sex</th>\n      <th>year_of_birth</th>\n      <th>Age_at_recruitment</th>\n      <th>ever_smoked</th>\n      <th>known_person_concerned_about_alcohol</th>\n      <th>ever_had_prolonged_sadness_or_depression</th>\n      <th>BMI</th>\n      <th>Genetic PCA</th>\n      <th>Diagnoses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1002940</td>\n      <td>1002940</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1940.0</td>\n      <td>67.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24.90000</td>\n      <td>-13.9614</td>\n      <td>['C61', 'J459', 'J61', 'K20', 'K219', 'K221', ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1003285</td>\n      <td>1003285</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1942.0</td>\n      <td>67.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27.70000</td>\n      <td>-12.2651</td>\n      <td>['C61', 'D508', 'H919', 'I849', 'K210', 'K227'...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1007944</td>\n      <td>1007944</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1948.0</td>\n      <td>60.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>26.94923</td>\n      <td>-10.5345</td>\n      <td>['G454', 'G560', 'G610', 'G819', 'G822', 'H819...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1014662</td>\n      <td>1014662</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1946.0</td>\n      <td>61.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>31.00000</td>\n      <td>-13.3167</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1015654</td>\n      <td>1015654</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1944.0</td>\n      <td>63.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36.60000</td>\n      <td>-11.4985</td>\n      <td>['A419', 'C446', 'E119', 'E872', 'F059', 'I10'...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}]}